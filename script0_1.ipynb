{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36364bit98989e7bed2f4a269ba49dc974f492ea",
   "display_name": "Python 3.6.3 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm \n",
    "import time\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import scipy\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing, metrics \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "from sklearn.utils import shuffle# 随机排列\n",
    "import gc\n",
    "import lightgbm as lgb \n",
    "import xgboost as xgb \n",
    "import joblib\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './data/eleme_round1_train_20200313/'\n",
    "test_path = './data/eleme_round1_testA_20200313/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据并加入date列\n",
    "def read_datafile(rootpath, section):\n",
    "    file_path = rootpath + section + '/'\n",
    "    data_list = []\n",
    "\n",
    "    for f in os.listdir(file_path):\n",
    "        date = f.split('.')[0].split('_')[1]\n",
    "        if section == 'action':\n",
    "            df = pd.read_csv(file_path+f,converters={'tracking_id':str})\n",
    "        elif section == 'order':\n",
    "            df = pd.read_csv(file_path+f,converters={'tracking_id':str})\n",
    "        elif section == 'courier':\n",
    "            df = pd.read_csv(file_path+f)\n",
    "        elif section == 'distance':\n",
    "            df = pd.read_csv(file_path+f,converters={'tracking_id':str, 'target_tracking_id':str})\n",
    "        df['date'] = date\n",
    "        data_list.append(df)\n",
    "    \n",
    "    return pd.concat(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majorid(df):\n",
    "    df['majorid'] = df['date'].map(str) + df['courier_id'].map(str) + '_' + df['wave_index'].map(str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropdate(df):\n",
    "    df.drop(['date', 'courier_id', 'wave_index'], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action的操作\n",
    "action_train = read_datafile(train_path, 'action')\n",
    "action_test = read_datafile(test_path, 'action')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_train = majorid(action_train)\n",
    "action_test = majorid(action_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_train_group(df):\n",
    "    groups = df.groupby(['majorid'])\n",
    "    df_future = []\n",
    "    df_last = []\n",
    "    for name, group in tqdm(groups):\n",
    "        future_data = group.tail(int(group.shape[0] * 0.55))\n",
    "        last_data = group.drop(future_data.index)\n",
    "\n",
    "        # last操作\n",
    "        last_data = last_data.tail(1)\n",
    "        last_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # 对future处理\n",
    "        future_data['label'] = 0\n",
    "        future_data.reset_index(drop=True, inplace=True)\n",
    "        future_data.loc[0,'label'] = 1# 标记正负样本\n",
    "\n",
    "        df_future.append(future_data)\n",
    "        df_last.append(last_data)\n",
    "    return_last = pd.concat(df_last)\n",
    "    return_future = pd.concat(df_future)\n",
    "    return_last.rename({'expect_time' : 'last_time'}, axis=1, inplace=True)# 把expecttime列重命名\n",
    "    return_future = shuffle(return_future)# 随机打乱顺序\n",
    "    return return_last, return_future\n",
    "\n",
    "def action_test_group(df):\n",
    "    groups = df.groupby(['majorid'])\n",
    "    df_future = []\n",
    "    df_last = []\n",
    "    for name, group in tqdm(groups):\n",
    "        future_data = group[group['expect_time']==0]\n",
    "        last_data = group.drop(future_data.index)\n",
    "\n",
    "        # last操作\n",
    "        last_data = last_data.tail(1)\n",
    "        last_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # future操作\n",
    "        future_data['label'] = None\n",
    "\n",
    "        df_future.append(future_data)\n",
    "        df_last.append(last_data)\n",
    "    return_last = pd.concat(df_last)\n",
    "    return_future = pd.concat(df_future)\n",
    "    return_last.rename({'expect_time' : 'last_time'}, axis=1, inplace=True)\n",
    "    return return_last, return_future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_train_thelast, action_train_future = action_train_group(action_train)\n",
    "action_test_thelast, action_test_future = action_test_group(action_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance数据读取\n",
    "distance_train = read_datafile(train_path, 'distance')\n",
    "distance_test = read_datafile(test_path, 'distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_train = majorid(distance_train)\n",
    "distance_test = majorid(distance_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_train = dropdate(distance_train)\n",
    "distance_test = dropdate(distance_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanlism_distance(df):\n",
    "    df['target_tan'] = (df['source_lat']-df['target_lat']) / (df['source_lng']-df['target_lng']) # df会自动处理出正无穷和负无穷，很秀\n",
    "    df['target_tan'] = np.arctan(df['target_tan'])\n",
    "    df['target_tan'] = np.degrees(df['target_tan'])\n",
    "    df['target_MHD'] = abs(df['source_lat']-df['target_lat']) + abs(df['source_lng']-df['target_lng'])# 加入曼哈顿距离\n",
    "\n",
    "    df.drop(['source_lat', 'target_lat', 'source_lng', 'target_lng'], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_train = tanlism_distance(distance_train)\n",
    "distance_test = tanlism_distance(distance_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_rule = {'source_type' : 'action_type'}\n",
    "\n",
    "distance_test.rename(rename_rule, axis=1, inplace=True)\n",
    "distance_train.rename(rename_rule, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train = pd.merge(left=action_train_thelast,right=distance_train, on=['majorid', 'tracking_id', 'action_type'], how='left')\n",
    "feature_test = pd.merge(left=action_test_thelast,right=distance_test, on=['majorid', 'tracking_id', 'action_type'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_rule = {'tracking_id':'last_tracking_id', 'action_type':'last_action_type', 'target_tracking_id':'tracking_id', 'target_type':'action_type'}\n",
    "\n",
    "feature_test.rename(rename_rule, axis=1, inplace=True)\n",
    "feature_train.rename(rename_rule, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_test.drop(['courier_wave_start_lng', 'courier_wave_start_lat'],axis=1,inplace=True)\n",
    "feature_train.drop(['courier_wave_start_lng', 'courier_wave_start_lat'],axis=1,inplace=True)\n",
    "feature_train = dropdate(feature_train)\n",
    "feature_test = dropdate(feature_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train = pd.merge(left=action_train_future,right=feature_train, on=['majorid', 'tracking_id', 'action_type'], how='left')\n",
    "feature_test = pd.merge(left=action_test_future,right=feature_test, on=['majorid', 'tracking_id', 'action_type'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del action_test, action_test_future, action_test_thelast\n",
    "del action_train, action_train_future, action_train_thelast\n",
    "del distance_test, distance_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取order\n",
    "order_train = read_datafile(train_path, 'order')\n",
    "order_test = read_datafile(test_path, 'order')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_test = majorid(order_test)\n",
    "order_train = majorid(order_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_test = dropdate(order_test)\n",
    "order_train = dropdate(order_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanlism_order(df):\n",
    "    df['delivery_tan'] = (df['deliver_lat']-df['pick_lat']) / (df['deliver_lng']-df['pick_lng'])\n",
    "    df['delivery_tan'] = np.arctan(df['delivery_tan'])\n",
    "    df['delivery_tan'] = np.degrees(df['delivery_tan'])\n",
    "    df['delivery_MHD'] = abs(df['deliver_lat']-df['pick_lat']) + abs(df['deliver_lng']-df['pick_lng'])# 加入曼哈顿距离\n",
    "\n",
    "    df.drop(['deliver_lat', 'pick_lat', 'deliver_lng', 'pick_lng'], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_test = tanlism_order(order_test)\n",
    "order_train = tanlism_order(order_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_test = pd.merge(left=feature_test, right=order_test, on=['majorid', 'tracking_id'], how='left')\n",
    "feature_train = pd.merge(left=feature_train, right=order_train, on=['majorid', 'tracking_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# courier操作\n",
    "courier_train = read_datafile(train_path, 'courier')\n",
    "courier_test = read_datafile(test_path, 'courier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_test = pd.merge(left=feature_test, right=courier_test, on=['courier_id', 'date'], how='left')\n",
    "feature_train = pd.merge(left=feature_train, right=courier_train, on=['courier_id', 'date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加入新的特征：id, rush和road\n",
    "feature_train['id'] = range(feature_train.shape[0])\n",
    "feature_test['id'] = range(feature_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rush(df):\n",
    "    df['rush'] = (df['last_time']-df['create_time']) / (df['promise_deliver_time']-df['create_time'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train = add_rush(feature_train)\n",
    "feature_test = add_rush(feature_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_road(df):\n",
    "    df['now'] = df['last_time'].apply(lambda x:time.strftime('%a_%H', time.localtime(x)))\n",
    "    df['is_holiday'] = df['now'].apply(lambda x: 1 if x.split('_')[0] in ['Sat', 'Sun'] else 0)\n",
    "    busytime = ['7', '8', '11', '12','17' ,'18']\n",
    "    normtime = ['5', '6', '9', '10', '13', '14', '15', '16', '19', '20', '21', '22']\n",
    "    df['road'] = df['now'].apply(lambda x: 1 if x.split('_')[1] in busytime else 2 if x.split('_')[1] in normtime else 3)\n",
    "    df.drop(['now'], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_test = add_road(feature_test)\n",
    "feature_train = add_road(feature_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather_grade转化为特征\n",
    "def weather(x):\n",
    "    if x == '正常天气':\n",
    "        x = 4\n",
    "    elif x == '轻微恶劣天气':\n",
    "        x = 3\n",
    "    elif x == '恶劣天气':\n",
    "        x = 2\n",
    "    elif x == '极恶劣天气':\n",
    "        x = 1\n",
    "    else:\n",
    "        x = 0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train['weather_grade'] = feature_train['weather_grade'].apply(lambda x: weather(x))\n",
    "feature_test['weather_grade'] = feature_test['weather_grade'].apply(lambda x: weather(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_test = pd.read_pickle('./temp/feature_test.pkl')\n",
    "feature_train = pd.read_pickle('./temp/feature_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加入expect_time-create_time和promise_deliver_time-expect_time作为预测的目标值\n",
    "feature_train['expect_used_time'] = feature_train['expect_time'] - feature_train['create_time']\n",
    "feature_train['will_residue_time'] = feature_train['promise_deliver_time'] - feature_train['expect_time']\n",
    "feature_test['expect_used_time'] = 0\n",
    "feature_test['will_residue_time'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_test.to_pickle('./temp/feature_test.pkl')\n",
    "feature_train.to_pickle('./temp/feature_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 异常值处理"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time \n",
    "import datetime\n",
    "from tqdm import tqdm \n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import scipy\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing, metrics \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import shuffle# 随机排列\n",
    "import gc\n",
    "import lightgbm as lgb \n",
    "import xgboost as xgb \n",
    "import joblib\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_test = pd.read_pickle('./temp/feature_test.pkl')\n",
    "feature_train = pd.read_pickle('./temp/feature_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 异常值处理的代码，可以随便调用。\n",
    "def outliers_proc(data, col_name, scale=3):\n",
    "    \"\"\"\n",
    "    用于清洗异常值，默认用 box_plot（scale=3）进行清洗\n",
    "    :param data: 接收 pandas 数据格式\n",
    "    :param col_name: pandas 列名\n",
    "    :param scale: 尺度\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    def box_plot_outliers(data_ser, box_scale):\n",
    "        \"\"\"\n",
    "        利用箱线图去除异常值\n",
    "        :param data_ser: 接收 pandas.Series 数据格式\n",
    "        :param box_scale: 箱线图尺度，\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        iqr = box_scale * (data_ser.quantile(0.75) - data_ser.quantile(0.25))\n",
    "        val_low = data_ser.quantile(0.25) - iqr\n",
    "        val_up = data_ser.quantile(0.75) + iqr\n",
    "        rule_low = (data_ser < val_low)\n",
    "        rule_up = (data_ser > val_up)\n",
    "        return (rule_low, rule_up), (val_low, val_up)\n",
    "\n",
    "    data_n = data.copy()\n",
    "    data_series = data_n[col_name]\n",
    "    rule, value = box_plot_outliers(data_series, box_scale=scale)\n",
    "    index = np.arange(data_series.shape[0])[rule[0] | rule[1]]\n",
    "    print(\"Delete number is: {}\".format(len(index)))\n",
    "    data_n = data_n.drop(index)\n",
    "    data_n.reset_index(drop=True, inplace=True)\n",
    "    print(\"Now column number is: {}\".format(data_n.shape[0]))\n",
    "    index_low = np.arange(data_series.shape[0])[rule[0]]\n",
    "    outliers = data_series.iloc[index_low]\n",
    "    print(\"Description of data less than the lower bound is:\")\n",
    "    print(pd.Series(outliers).describe())\n",
    "    index_up = np.arange(data_series.shape[0])[rule[1]]\n",
    "    outliers = data_series.iloc[index_up]\n",
    "    print(\"Description of data larger than the upper bound is:\")\n",
    "    print(pd.Series(outliers).describe())\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 7))\n",
    "    sns.boxplot(y=data[col_name], data=data, palette=\"Set1\", ax=ax[0])\n",
    "    sns.boxplot(y=data_n[col_name], data=data_n, palette=\"Set1\", ax=ax[1])\n",
    "    return data_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下边是实际需要运行的\n",
    "feature_train = outliers_proc(feature_train, 'expect_used_time')\n",
    "feature_train = outliers_proc(feature_train, 'will_residue_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_test.to_pickle('./temp/feature_test_reg.pkl')\n",
    "feature_train.to_pickle('./temp/feature_train_reg.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 回归任务"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time \n",
    "import datetime\n",
    "from tqdm import tqdm \n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import scipy\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing, metrics \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import shuffle# 随机排列\n",
    "import gc\n",
    "import lightgbm as lgb \n",
    "import xgboost as xgb \n",
    "import joblib\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_test = pd.read_pickle('./temp/feature_test_reg.pkl')\n",
    "feature_train = pd.read_pickle('./temp/feature_train_reg.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_prediction = feature_test\n",
    "reg_prediction['expect_time'] = 0\n",
    "reg_prediction['expect_used_time'] = 0\n",
    "reg_prediction['will_residue_time'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 建立时间预测的回归任务1\n",
    "# y_col = 'expect_used_time'\n",
    "# x_col = ['weather_grade', 'level', 'speed', 'max_load', 'is_holiday', 'rush', 'road', 'grid_distance', 'target_tan', 'delivery_tan', 'delivery_MHD', 'target_MHD']# 加入expect_used_time和will_residue_time之后的x_col\n",
    "\n",
    "# t0 = time.time()\n",
    "# model = lgb.LGBMRegressor(\n",
    "#     metric = 'mae',\n",
    "#     num_leaves = 50,\n",
    "#     max_depth = 7,\n",
    "#     n_estimators = 10000000,\n",
    "#     learning_rate = 0.1,\n",
    "#     bagging_fraction = 1,\n",
    "#     feature_fraction = 1,\n",
    "#     reg_alpha = 0,\n",
    "#     reg_lambda = 1\n",
    "# )\n",
    "\n",
    "# valueK = 10\n",
    "# oof = []\n",
    "# df_importance_list = []\n",
    "\n",
    "# kfold = KFold(n_splits=valueK, shuffle=True, random_state=2020)\n",
    "# for fold_id, (trn_idx, val_idx) in enumerate(kfold.split(feature_train[x_col], feature_train[y_col])):\n",
    "#     X_train = feature_train.iloc[trn_idx][x_col]\n",
    "#     Y_train = feature_train.iloc[trn_idx][y_col]\n",
    "\n",
    "#     X_val = feature_train.iloc[val_idx][x_col]\n",
    "#     Y_val = feature_train.iloc[val_idx][y_col]\n",
    "\n",
    "#     print('\\nFold{} Training ======================================\\n'.format(fold_id+1))\n",
    "\n",
    "#     lgb_model = model.fit(\n",
    "#         X_train,\n",
    "#         Y_train,\n",
    "#         eval_names=['train', 'valid'],\n",
    "#         eval_set=[(X_train, Y_train), (X_val, Y_val)],\n",
    "#         verbose=500,\n",
    "#         eval_metric='mae',\n",
    "#         early_stopping_rounds=100\n",
    "#     )\n",
    "\n",
    "#     pred_val = lgb_model.predict(X_val, num_iteration=lgb_model.best_iteration_)\n",
    "#     df_oof = feature_train.iloc[val_idx][['id', y_col]].copy()\n",
    "#     df_oof['pred'] = pred_val\n",
    "#     oof.append(df_oof)\n",
    "\n",
    "#     pred_test = lgb_model.predict(feature_test[x_col], num_iteration=lgb_model.best_iteration_)\n",
    "#     reg_prediction['expect_used_time'] += (pred_test/valueK)\n",
    "\n",
    "#     df_importance = pd.DataFrame({\n",
    "#         'column': x_col,\n",
    "#         'importance': lgb_model.feature_importances_\n",
    "#     })\n",
    "#     df_importance_list.append(df_importance)\n",
    "#     # break\n",
    "\n",
    "#     # del lgb_model, pred_val, pred_test, X_train, Y_train, X_val, Y_val\n",
    "#     # gc.collect()\n",
    "# t1 = time.time()\n",
    "# print('end train, use{} second'.format(t1-t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_oof = pd.concat(oof)\n",
    "# mae = metrics.mean_absolute_error(df_oof[y_col], df_oof['pred'])\n",
    "# print('mae:', mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_importance = pd.concat(df_importance_list)\n",
    "# df_importance = df_importance.groupby(['column'])['importance'].agg('mean').sort_values(ascending=False).reset_index()\n",
    "# df_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立时间预测的回归任务2\n",
    "y_col = 'will_residue_time'\n",
    "# x_col = ['weather_grade', 'level', 'speed', 'max_load', 'is_holiday', 'rush', 'road', 'grid_distance', 'target_tan', 'delivery_tan', 'delivery_MHD', 'target_MHD']# 加入expect_used_time和will_residue_time之后的x_col\n",
    "# x_col = ['weather_grade', 'level', 'speed', 'max_load', 'rush', 'road', 'grid_distance', 'delivery_MHD', 'target_MHD']\n",
    "x_col = ['speed', 'rush', 'grid_distance', 'delivery_MHD', 'target_MHD']\n",
    "\n",
    "\n",
    "t0 = time.time()\n",
    "model = lgb.LGBMRegressor(\n",
    "    metric = 'mae',\n",
    "    num_leaves = 64,\n",
    "    max_depth = 7,\n",
    "    n_estimators = 300,\n",
    "    learning_rate = 0.1,\n",
    "    bagging_fraction = 1,\n",
    "    feature_fraction = 0.8,\n",
    "    reg_alpha = 0,\n",
    "    reg_lambda = 0\n",
    ")\n",
    "\n",
    "valueK = 10\n",
    "oof = []\n",
    "df_importance_list = []\n",
    "\n",
    "kfold = KFold(n_splits=valueK, shuffle=True, random_state=2020)\n",
    "for fold_id, (trn_idx, val_idx) in enumerate(kfold.split(feature_train[x_col], feature_train[y_col])):\n",
    "    X_train = feature_train.iloc[trn_idx][x_col]\n",
    "    Y_train = feature_train.iloc[trn_idx][y_col]\n",
    "\n",
    "    X_val = feature_train.iloc[val_idx][x_col]\n",
    "    Y_val = feature_train.iloc[val_idx][y_col]\n",
    "\n",
    "    print('\\nFold{} Training ======================================\\n'.format(fold_id+1))\n",
    "\n",
    "    lgb_model = model.fit(\n",
    "        X_train,\n",
    "        Y_train,\n",
    "        eval_names=['train', 'valid'],\n",
    "        eval_set=[(X_train, Y_train), (X_val, Y_val)],\n",
    "        verbose=500,\n",
    "        eval_metric='mae',\n",
    "        early_stopping_rounds=100\n",
    "    )\n",
    "\n",
    "    pred_val = lgb_model.predict(X_val, num_iteration=lgb_model.best_iteration_)\n",
    "    df_oof = feature_train.iloc[val_idx][['id', y_col]].copy()\n",
    "    df_oof['pred'] = pred_val\n",
    "    oof.append(df_oof)\n",
    "\n",
    "    pred_test = lgb_model.predict(feature_test[x_col], num_iteration=lgb_model.best_iteration_)\n",
    "    reg_prediction['will_residue_time'] += (pred_test/valueK)\n",
    "\n",
    "    df_importance = pd.DataFrame({\n",
    "        'column': x_col,\n",
    "        'importance': lgb_model.feature_importances_\n",
    "    })\n",
    "    df_importance_list.append(df_importance)\n",
    "    # break\n",
    "\n",
    "    # del lgb_model, pred_val, pred_test, X_train, Y_train, X_val, Y_val\n",
    "    # gc.collect()\n",
    "t1 = time.time()\n",
    "print('end train, use{} second'.format(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oof = pd.concat(oof)\n",
    "mae = metrics.mean_absolute_error(df_oof[y_col], df_oof['pred'])\n",
    "print('mae:', mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 还原expect_time\n",
    "# reg_prediction['expect_time'] = ((reg_prediction['create_time']+reg_prediction['expect_used_time']) + (reg_prediction['promise_deliver_time']-reg_prediction['will_residue_time'])) / 2\n",
    "reg_prediction['expect_time'] = reg_prediction['promise_deliver_time'] - reg_prediction['will_residue_time']# 只使用后一个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train.to_pickle('./temp/regfuture_train_regend.pkl')\n",
    "reg_prediction.to_pickle('./temp/regfuture_test_regend.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分类特征"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time \n",
    "import datetime\n",
    "from tqdm import tqdm \n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import scipy\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing, metrics \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import shuffle# 随机排列\n",
    "import gc\n",
    "import lightgbm as lgb \n",
    "import xgboost as xgb \n",
    "import joblib\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train = pd.read_pickle('./temp/regfuture_train_regend.pkl')\n",
    "feature_test = pd.read_pickle('./temp/regfuture_test_regend.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征 deadline\n",
    "def deadLine(df):\n",
    "    df['deadline'] = df['promise_deliver_time'] - df['expect_time']\n",
    "    df['need_speed'] = df['grid_distance'] / df['deadline']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train = deadLine(feature_train)\n",
    "feature_test = deadLine(feature_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 异常值处理的代码，可以随便调用。\n",
    "def outliers_proc(data, col_name, scale=3):\n",
    "    \"\"\"\n",
    "    用于清洗异常值，默认用 box_plot（scale=3）进行清洗\n",
    "    :param data: 接收 pandas 数据格式\n",
    "    :param col_name: pandas 列名\n",
    "    :param scale: 尺度\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    def box_plot_outliers(data_ser, box_scale):\n",
    "        \"\"\"\n",
    "        利用箱线图去除异常值\n",
    "        :param data_ser: 接收 pandas.Series 数据格式\n",
    "        :param box_scale: 箱线图尺度，\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        iqr = box_scale * (data_ser.quantile(0.75) - data_ser.quantile(0.25))\n",
    "        val_low = data_ser.quantile(0.25) - iqr\n",
    "        val_up = data_ser.quantile(0.75) + iqr\n",
    "        rule_low = (data_ser < val_low)\n",
    "        rule_up = (data_ser > val_up)\n",
    "        return (rule_low, rule_up), (val_low, val_up)\n",
    "\n",
    "    data_n = data.copy()\n",
    "    data_series = data_n[col_name]\n",
    "    rule, value = box_plot_outliers(data_series, box_scale=scale)\n",
    "    index = np.arange(data_series.shape[0])[rule[0] | rule[1]]\n",
    "    print(\"Delete number is: {}\".format(len(index)))\n",
    "    data_n = data_n.drop(index)\n",
    "    data_n.reset_index(drop=True, inplace=True)\n",
    "    print(\"Now column number is: {}\".format(data_n.shape[0]))\n",
    "    index_low = np.arange(data_series.shape[0])[rule[0]]\n",
    "    outliers = data_series.iloc[index_low]\n",
    "    print(\"Description of data less than the lower bound is:\")\n",
    "    print(pd.Series(outliers).describe())\n",
    "    index_up = np.arange(data_series.shape[0])[rule[1]]\n",
    "    outliers = data_series.iloc[index_up]\n",
    "    print(\"Description of data larger than the upper bound is:\")\n",
    "    print(pd.Series(outliers).describe())\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 7))\n",
    "    sns.boxplot(y=data[col_name], data=data, palette=\"Set1\", ax=ax[0])\n",
    "    sns.boxplot(y=data_n[col_name], data=data_n, palette=\"Set1\", ax=ax[1])\n",
    "    return data_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train = outliers_proc(feature_train, 'need_speed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train['need_speed'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_Picked(df):\n",
    "    df['is_picked'] = df['last_time'] - df['estimate_pick_time']\n",
    "    df['is_picked'] = df['is_picked'].apply(lambda x: 0 if x<0 else 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_test = is_Picked(feature_test)\n",
    "feature_train = is_Picked(feature_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train.to_pickle('./temp/future_train_clf.pkl')\n",
    "feature_test.to_pickle('./temp/future_test_clf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分类模型"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time \n",
    "import datetime\n",
    "from tqdm import tqdm \n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import scipy\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing, metrics \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import shuffle# 随机排列\n",
    "import gc\n",
    "import lightgbm as lgb \n",
    "import xgboost as xgb \n",
    "import joblib\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train = pd.read_pickle('./temp/future_train_clf.pkl')\n",
    "feature_test = pd.read_pickle('./temp/future_test_clf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 243158 entries, 0 to 243157\nData columns (total 38 columns):\ncourier_id                243158 non-null int64\nwave_index                243158 non-null int64\ntracking_id               243158 non-null object\ncourier_wave_start_lng    243158 non-null float64\ncourier_wave_start_lat    243158 non-null float64\naction_type               243158 non-null object\nexpect_time               243158 non-null int64\ndate                      243158 non-null object\nmajorid                   243158 non-null object\nlabel                     243158 non-null int64\nlast_tracking_id          243158 non-null object\nlast_action_type          243158 non-null object\nlast_time                 243158 non-null int64\ngrid_distance             243158 non-null float64\ntarget_tan                237516 non-null float64\ntarget_MHD                243158 non-null float64\nweather_grade             243158 non-null int64\ncreate_time               243158 non-null int64\nconfirm_time              243158 non-null int64\nassigned_time             243158 non-null int64\npromise_deliver_time      243158 non-null int64\nestimate_pick_time        243158 non-null int64\naoi_id                    243158 non-null object\nshop_id                   243158 non-null object\ndelivery_tan              243111 non-null float64\ndelivery_MHD              243158 non-null float64\nlevel                     243158 non-null int64\nspeed                     243158 non-null float64\nmax_load                  243158 non-null int64\nid                        243158 non-null int32\nrush                      243158 non-null float64\nis_holiday                243158 non-null int64\nroad                      243158 non-null int64\nexpect_used_time          243158 non-null int64\nwill_residue_time         243158 non-null int64\ndeadline                  243158 non-null int64\nneed_speed                243158 non-null float64\nis_picked                 243158 non-null int64\ndtypes: float64(10), int32(1), int64(19), object(8)\nmemory usage: 69.6+ MB\n"
    }
   ],
   "source": [
    "feature_train.info()# needspeed和speed是不是取个比值更好？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speed_Rush(df):\n",
    "    df['speed_rush'] = df['speed'] - df['need_speed']\n",
    "    return df \n",
    "feature_test = speed_Rush(feature_test)\n",
    "feature_train = speed_Rush(feature_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = feature_test[['courier_id', 'wave_index', 'tracking_id', 'courier_wave_start_lng', 'courier_wave_start_lat', 'action_type', 'expect_time', 'date', 'id', 'majorid', 'label']]\n",
    "prediction['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_col = 'label'\n",
    "# x_col = ['grid_distance', 'target_tan', 'target_MHD', 'weather_grade', 'aoi_id', 'shop_id', 'delivery_tan', 'delivery_MHD', 'level', 'speed', 'max_load', 'rush', 'road', 'expect_used_time', 'will_residue_time', 'deadline', 'need_speed', 'is_picked']\n",
    "x_col = ['grid_distance', 'target_tan', 'target_MHD', 'weather_grade', 'delivery_tan', 'delivery_MHD', 'level', 'speed', 'max_load', 'rush', 'road', 'expect_used_time', 'will_residue_time', 'deadline', 'need_speed', 'is_picked', 'speed_rush']\n",
    "\n",
    "model = lgb.LGBMClassifier(num_leaves=50,\n",
    "                           max_depth=7,\n",
    "                           learning_rate=0.03,\n",
    "                           n_estimators=100000,\n",
    "                           subsample=0.8,\n",
    "                           feature_fraction=0.8,\n",
    "                           reg_alpha=0.8,\n",
    "                           reg_lambda=0.8,\n",
    "                           random_state=2020,\n",
    "                           metric=None\n",
    "                           )\n",
    "\n",
    "\n",
    "oof = []\n",
    "df_importance_list = []\n",
    "\n",
    "kfold = GroupKFold(n_splits=10)\n",
    "for fold_id, (trn_idx, val_idx) in enumerate(kfold.split(feature_train[x_col], feature_train[y_col], feature_train['majorid'])):\n",
    "    X_train = feature_train.iloc[trn_idx][x_col]\n",
    "    Y_train = feature_train.iloc[trn_idx][y_col]\n",
    "\n",
    "    X_val = feature_train.iloc[val_idx][x_col]\n",
    "    Y_val = feature_train.iloc[val_idx][y_col]\n",
    "\n",
    "    print('\\nFold_{} Training ================================\\n'.format(fold_id+1))\n",
    "\n",
    "    lgb_model = model.fit(\n",
    "        X_train,\n",
    "        Y_train,\n",
    "        eval_names=['train', 'valid'],\n",
    "        eval_set=[(X_train, Y_train), (X_val, Y_val)],\n",
    "        verbose=200,\n",
    "        eval_metric='auc',\n",
    "        early_stopping_rounds=100\n",
    "    )\n",
    "\n",
    "    pred_val = lgb_model.predict_proba(X_val, num_iteration=lgb_model.best_iteration_)[:, 1]\n",
    "    df_oof = feature_train.iloc[val_idx][['id', 'majorid', y_col]].copy()\n",
    "    df_oof['pred'] = pred_val\n",
    "    oof.append(df_oof)\n",
    "\n",
    "    pred_test = lgb_model.predict_proba(feature_test[x_col], num_iteration=lgb_model.best_iteration_)[:, 1]\n",
    "    prediction['label'] += pred_test / 10\n",
    "\n",
    "    df_importance = pd.DataFrame({\n",
    "        'column': x_col,\n",
    "        'importance': lgb_model.feature_importances_,\n",
    "    })\n",
    "    df_importance_list.append(df_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance = pd.concat(df_importance_list)\n",
    "df_importance = df_importance.groupby(['column'])['importance'].agg('mean').sort_values(ascending=False).reset_index()\n",
    "df_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wave_label_func(group):\n",
    "    target_list = group['label'].values.tolist()\n",
    "    pred_list = group['pred'].values.tolist()\n",
    "    max_index = pred_list.index(max(pred_list))\n",
    "    if target_list[max_index] == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oof = pd.concat(oof)\n",
    "df_temp = df_oof.groupby(['majorid']).apply(wave_label_func).reset_index()\n",
    "df_temp.columns = ['majorid', 'label']\n",
    "acc = df_temp[df_temp['label'] == 1].shape[0] / df_temp.shape[0]\n",
    "print('acc:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_func(majorid):\n",
    "    majorid = majorid.values.tolist()\n",
    "    max_index = majorid.index(max(majorid))\n",
    "    result = np.zeros(len(majorid))\n",
    "    result[max_index] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction['rusult'] = prediction.groupby(['majorid'])['label'].transform(result_func)\n",
    "subfile = prediction[prediction['rusult'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = subfile[['courier_id', 'wave_index', 'tracking_id', 'courier_wave_start_lng', 'courier_wave_start_lat', 'action_type', 'expect_time', 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "os.makedirs('./sub/{}'.format('result'), exist_ok=True)\n",
    "f = zipfile.ZipFile('./sub/{}.zip'.format('result'), 'w', zipfile.ZIP_DEFLATED)\n",
    "for date in result['date'].unique():\n",
    "    df_temp = result[prediction['date'] == date]\n",
    "    del df_temp['date']\n",
    "    df_temp.to_csv('./sub/{}/action_{}.txt'.format('result', date), index=False)\n",
    "    f.write('./sub/{}/action_{}.txt'.format('result', date), 'action_{}.txt'.format(date))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}