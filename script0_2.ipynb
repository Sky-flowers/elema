{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36364bit98989e7bed2f4a269ba49dc974f492ea",
   "display_name": "Python 3.6.3 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm \n",
    "import time\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import scipy\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing, metrics \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "from sklearn.utils import shuffle# 随机排列\n",
    "import gc\n",
    "import lightgbm as lgb \n",
    "import xgboost as xgb \n",
    "import joblib\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './data/eleme_round1_train_20200313/'\n",
    "test_path = './data/eleme_round1_testA_20200313/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据并加入date列\n",
    "def read_datafile(rootpath, section):\n",
    "    file_path = rootpath + section + '/'\n",
    "    data_list = []\n",
    "\n",
    "    for f in os.listdir(file_path):\n",
    "        date = f.split('.')[0].split('_')[1]\n",
    "        if section == 'action':\n",
    "            df = pd.read_csv(file_path+f,converters={'tracking_id':str})\n",
    "        elif section == 'order':\n",
    "            df = pd.read_csv(file_path+f,converters={'tracking_id':str})\n",
    "        elif section == 'courier':\n",
    "            df = pd.read_csv(file_path+f)\n",
    "        elif section == 'distance':\n",
    "            df = pd.read_csv(file_path+f,converters={'tracking_id':str, 'target_tracking_id':str})\n",
    "        df['date'] = date\n",
    "        data_list.append(df)\n",
    "    \n",
    "    return pd.concat(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majorid(df):\n",
    "    df['majorid'] = df['date'].map(str) + df['courier_id'].map(str) + '_' + df['wave_index'].map(str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropdate(df):\n",
    "    df.drop(['date', 'courier_id', 'wave_index'], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action的操作\n",
    "action_train = read_datafile(train_path, 'action')\n",
    "action_test = read_datafile(test_path, 'action')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_train = majorid(action_train)\n",
    "action_test = majorid(action_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_train_group(df):\n",
    "    groups = df.groupby(['majorid'])\n",
    "    df_future = []\n",
    "    df_last = []\n",
    "    for name, group in tqdm(groups):\n",
    "        future_data = group.tail(int(group.shape[0] * 0.55))\n",
    "        last_data = group.drop(future_data.index)\n",
    "\n",
    "        # last操作\n",
    "        last_data = last_data.tail(1)\n",
    "        last_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # 对future处理\n",
    "        future_data['label'] = 0\n",
    "        future_data.reset_index(drop=True, inplace=True)\n",
    "        future_data.loc[0,'label'] = 1# 标记正负样本\n",
    "\n",
    "        df_future.append(future_data)\n",
    "        df_last.append(last_data)\n",
    "    return_last = pd.concat(df_last)\n",
    "    return_future = pd.concat(df_future)\n",
    "    return_last.rename({'expect_time' : 'last_time'}, axis=1, inplace=True)# 把expecttime列重命名\n",
    "    return_future = shuffle(return_future)# 随机打乱顺序\n",
    "    return return_last, return_future\n",
    "\n",
    "def action_test_group(df):\n",
    "    groups = df.groupby(['majorid'])\n",
    "    df_future = []\n",
    "    df_last = []\n",
    "    for name, group in tqdm(groups):\n",
    "        future_data = group[group['expect_time']==0]\n",
    "        last_data = group.drop(future_data.index)\n",
    "\n",
    "        # last操作\n",
    "        last_data = last_data.tail(1)\n",
    "        last_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # future操作\n",
    "        future_data['label'] = None\n",
    "\n",
    "        df_future.append(future_data)\n",
    "        df_last.append(last_data)\n",
    "    return_last = pd.concat(df_last)\n",
    "    return_future = pd.concat(df_future)\n",
    "    return_last.rename({'expect_time' : 'last_time'}, axis=1, inplace=True)\n",
    "    return return_last, return_future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_train_thelast, action_train_future = action_train_group(action_train)\n",
    "action_test_thelast, action_test_future = action_test_group(action_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance数据读取\n",
    "distance_train = read_datafile(train_path, 'distance')\n",
    "distance_test = read_datafile(test_path, 'distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_train = majorid(distance_train)\n",
    "distance_test = majorid(distance_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_train = dropdate(distance_train)\n",
    "distance_test = dropdate(distance_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanlism_distance(df):\n",
    "    df['target_tan'] = (df['source_lat']-df['target_lat']) / (df['source_lng']-df['target_lng']) # df会自动处理出正无穷和负无穷，很秀\n",
    "    df['target_tan'] = np.arctan(df['target_tan'])\n",
    "    df['target_tan'] = np.degrees(df['target_tan'])\n",
    "    df['target_MHD'] = abs(df['source_lat']-df['target_lat']) + abs(df['source_lng']-df['target_lng'])# 加入曼哈顿距离\n",
    "\n",
    "    df.drop(['source_lat', 'target_lat', 'source_lng', 'target_lng'], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_train = tanlism_distance(distance_train)\n",
    "distance_test = tanlism_distance(distance_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_rule = {'source_type' : 'action_type'}\n",
    "\n",
    "distance_test.rename(rename_rule, axis=1, inplace=True)\n",
    "distance_train.rename(rename_rule, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train = pd.merge(left=action_train_thelast,right=distance_train, on=['majorid', 'tracking_id', 'action_type'], how='left')\n",
    "feature_test = pd.merge(left=action_test_thelast,right=distance_test, on=['majorid', 'tracking_id', 'action_type'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_rule = {'tracking_id':'last_tracking_id', 'action_type':'last_action_type', 'target_tracking_id':'tracking_id', 'target_type':'action_type'}\n",
    "\n",
    "feature_test.rename(rename_rule, axis=1, inplace=True)\n",
    "feature_train.rename(rename_rule, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_test.drop(['courier_wave_start_lng', 'courier_wave_start_lat'],axis=1,inplace=True)\n",
    "feature_train.drop(['courier_wave_start_lng', 'courier_wave_start_lat'],axis=1,inplace=True)\n",
    "feature_train = dropdate(feature_train)\n",
    "feature_test = dropdate(feature_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train = pd.merge(left=action_train_future,right=feature_train, on=['majorid', 'tracking_id', 'action_type'], how='left')\n",
    "feature_test = pd.merge(left=action_test_future,right=feature_test, on=['majorid', 'tracking_id', 'action_type'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del action_test, action_test_future, action_test_thelast\n",
    "del action_train, action_train_future, action_train_thelast\n",
    "del distance_test, distance_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取order\n",
    "order_train = read_datafile(train_path, 'order')\n",
    "order_test = read_datafile(test_path, 'order')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_test = majorid(order_test)\n",
    "order_train = majorid(order_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_test = dropdate(order_test)\n",
    "order_train = dropdate(order_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanlism_order(df):\n",
    "    df['delivery_tan'] = (df['deliver_lat']-df['pick_lat']) / (df['deliver_lng']-df['pick_lng'])\n",
    "    df['delivery_tan'] = np.arctan(df['delivery_tan'])\n",
    "    df['delivery_tan'] = np.degrees(df['delivery_tan'])\n",
    "    df['delivery_MHD'] = abs(df['deliver_lat']-df['pick_lat']) + abs(df['deliver_lng']-df['pick_lng'])# 加入曼哈顿距离\n",
    "\n",
    "    df.drop(['deliver_lat', 'pick_lat', 'deliver_lng', 'pick_lng'], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_test = tanlism_order(order_test)\n",
    "order_train = tanlism_order(order_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_test = pd.merge(left=feature_test, right=order_test, on=['majorid', 'tracking_id'], how='left')\n",
    "feature_train = pd.merge(left=feature_train, right=order_train, on=['majorid', 'tracking_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# courier操作\n",
    "courier_train = read_datafile(train_path, 'courier')\n",
    "courier_test = read_datafile(test_path, 'courier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_test = pd.merge(left=feature_test, right=courier_test, on=['courier_id', 'date'], how='left')\n",
    "feature_train = pd.merge(left=feature_train, right=courier_train, on=['courier_id', 'date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加入新的特征：id, rush和road\n",
    "feature_train['id'] = range(feature_train.shape[0])\n",
    "feature_test['id'] = range(feature_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rush(df):\n",
    "    df['rush'] = (df['last_time']-df['create_time']) / (df['promise_deliver_time']-df['create_time'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train = add_rush(feature_train)\n",
    "feature_test = add_rush(feature_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_road(df):\n",
    "    df['now'] = df['last_time'].apply(lambda x:time.strftime('%a_%H', time.localtime(x)))\n",
    "    df['is_holiday'] = df['now'].apply(lambda x: 1 if x.split('_')[0] in ['Sat', 'Sun'] else 0)\n",
    "    busytime = ['7', '8', '11', '12','17' ,'18']\n",
    "    normtime = ['5', '6', '9', '10', '13', '14', '15', '16', '19', '20', '21', '22']\n",
    "    df['road'] = df['now'].apply(lambda x: 1 if x.split('_')[1] in busytime else 2 if x.split('_')[1] in normtime else 3)\n",
    "    df.drop(['now'], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_test = add_road(feature_test)\n",
    "feature_train = add_road(feature_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather_grade转化为特征\n",
    "def weather(x):\n",
    "    if x == '正常天气':\n",
    "        x = 4\n",
    "    elif x == '轻微恶劣天气':\n",
    "        x = 3\n",
    "    elif x == '恶劣天气':\n",
    "        x = 2\n",
    "    elif x == '极恶劣天气':\n",
    "        x = 1\n",
    "    else:\n",
    "        x = 0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train['weather_grade'] = feature_train['weather_grade'].apply(lambda x: weather(x))\n",
    "feature_test['weather_grade'] = feature_test['weather_grade'].apply(lambda x: weather(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_test = pd.read_pickle('./temp/feature_test.pkl')\n",
    "feature_train = pd.read_pickle('./temp/feature_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加入expect_time-create_time和promise_deliver_time-expect_time作为预测的目标值\n",
    "feature_train['expect_used_time'] = feature_train['expect_time'] - feature_train['create_time']\n",
    "feature_train['will_residue_time'] = feature_train['promise_deliver_time'] - feature_train['expect_time']\n",
    "feature_test['expect_used_time'] = 0\n",
    "feature_test['will_residue_time'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_test.to_pickle('./temp/feature_test.pkl')\n",
    "feature_train.to_pickle('./temp/feature_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 异常值处理"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time \n",
    "import datetime\n",
    "from tqdm import tqdm \n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import scipy\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing, metrics \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import shuffle# 随机排列\n",
    "import gc\n",
    "import lightgbm as lgb \n",
    "import xgboost as xgb \n",
    "import joblib\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_test = pd.read_pickle('./temp/feature_test.pkl')\n",
    "feature_train = pd.read_pickle('./temp/feature_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新包装了一个异常值处理函数，把异常值清洗设为50%中位数\n",
    "def deal_outliers(df, col):\n",
    "    # df = feature_train\n",
    "    # col = 'expect_used_time'\n",
    "    def Box_outliers(data_ser):\n",
    "        iqr = 3 * (data_ser.quantile(0.75) - data_ser.quantile(0.25))\n",
    "        val_low = data_ser.quantile(0.25) - iqr\n",
    "        val_up = data_ser.quantile(0.75) + iqr\n",
    "        return val_low, val_up\n",
    "\n",
    "    data_ser = df[col]\n",
    "    val_low, val_up = Box_outliers(df[col])\n",
    "    std_data = df[col].quantile(0.5)\n",
    "    df[col] = df[col].apply(lambda x: std_data if x < val_low else std_data if x > val_up else x)\n",
    "\n",
    "    sns.boxplot(y=df[col], data=df, palette=\"Set1\")\n",
    "    return df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# 下边是实际需要运行的\n",
    "feature_train['expect_used_time'] = deal_outliers(feature_train, 'expect_used_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train['will_residue_time'] = deal_outliers(feature_train, 'will_residue_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train['expect_used_time'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_test.to_pickle('./temp/feature_test_reg.pkl')\n",
    "feature_train.to_pickle('./temp/feature_train_reg.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 回归任务"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time \n",
    "import datetime\n",
    "from tqdm import tqdm \n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import scipy\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing, metrics \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import shuffle# 随机排列\n",
    "import gc\n",
    "import lightgbm as lgb \n",
    "import xgboost as xgb \n",
    "import joblib\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_test = pd.read_pickle('./temp/feature_test_reg.pkl')\n",
    "feature_train = pd.read_pickle('./temp/feature_train_reg.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_prediction = feature_test\n",
    "reg_prediction['expect_time'] = 0\n",
    "reg_prediction['expect_used_time'] = 0\n",
    "reg_prediction['will_residue_time'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立时间预测的回归任务1\n",
    "y_col = 'expect_used_time'\n",
    "# x_col = ['weather_grade', 'level', 'speed', 'max_load', 'is_holiday', 'rush', 'road', 'grid_distance', 'target_tan', 'delivery_tan', 'delivery_MHD', 'target_MHD']# 加入expect_used_time和will_residue_time之后的x_col\n",
    "x_col = ['weather_grade', 'level', 'speed', 'max_load', 'is_holiday', 'rush', 'road', 'grid_distance', 'delivery_MHD', 'target_MHD']\n",
    "\n",
    "t0 = time.time()\n",
    "model = lgb.LGBMRegressor(\n",
    "    metric = 'mae',\n",
    "    num_leaves = 50,\n",
    "    max_depth = 7,\n",
    "    n_estimators = 10000000,\n",
    "    learning_rate = 0.1,\n",
    "    bagging_fraction = 1,\n",
    "    feature_fraction = 1,\n",
    "    reg_alpha = 0,\n",
    "    reg_lambda = 1\n",
    ")\n",
    "\n",
    "valueK = 10\n",
    "oof = []\n",
    "df_importance_list = []\n",
    "\n",
    "kfold = KFold(n_splits=valueK, shuffle=True, random_state=2020)\n",
    "for fold_id, (trn_idx, val_idx) in enumerate(kfold.split(feature_train[x_col], feature_train[y_col])):\n",
    "    X_train = feature_train.iloc[trn_idx][x_col]\n",
    "    Y_train = feature_train.iloc[trn_idx][y_col]\n",
    "\n",
    "    X_val = feature_train.iloc[val_idx][x_col]\n",
    "    Y_val = feature_train.iloc[val_idx][y_col]\n",
    "\n",
    "    print('\\nFold{} Training ======================================\\n'.format(fold_id+1))\n",
    "\n",
    "    lgb_model = model.fit(\n",
    "        X_train,\n",
    "        Y_train,\n",
    "        eval_names=['train', 'valid'],\n",
    "        eval_set=[(X_train, Y_train), (X_val, Y_val)],\n",
    "        verbose=500,\n",
    "        eval_metric='mae',\n",
    "        early_stopping_rounds=100\n",
    "    )\n",
    "\n",
    "    pred_val = lgb_model.predict(X_val, num_iteration=lgb_model.best_iteration_)\n",
    "    df_oof = feature_train.iloc[val_idx][['id', y_col]].copy()\n",
    "    df_oof['pred'] = pred_val\n",
    "    oof.append(df_oof)\n",
    "\n",
    "    pred_test = lgb_model.predict(feature_test[x_col], num_iteration=lgb_model.best_iteration_)\n",
    "    reg_prediction['expect_used_time'] += (pred_test/valueK)\n",
    "\n",
    "    df_importance = pd.DataFrame({\n",
    "        'column': x_col,\n",
    "        'importance': lgb_model.feature_importances_\n",
    "    })\n",
    "    df_importance_list.append(df_importance)\n",
    "    # break\n",
    "\n",
    "    # del lgb_model, pred_val, pred_test, X_train, Y_train, X_val, Y_val\n",
    "    # gc.collect()\n",
    "t1 = time.time()\n",
    "print('end train, use{} second'.format(t1-t0))\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oof = pd.concat(oof)\n",
    "mae = metrics.mean_absolute_error(df_oof[y_col], df_oof['pred'])\n",
    "print('mae:', mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_importance = pd.concat(df_importance_list)\n",
    "# df_importance = df_importance.groupby(['column'])['importance'].agg('mean').sort_values(ascending=False).reset_index()\n",
    "# df_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立时间预测的回归任务2\n",
    "y_col = 'will_residue_time'\n",
    "# x_col = ['weather_grade', 'level', 'speed', 'max_load', 'is_holiday', 'rush', 'road', 'grid_distance', 'target_tan', 'delivery_tan', 'delivery_MHD', 'target_MHD']# 加入expect_used_time和will_residue_time之后的x_col\n",
    "x_col = ['weather_grade', 'level', 'speed', 'max_load', 'is_holiday', 'rush', 'road', 'grid_distance', 'delivery_MHD', 'target_MHD']\n",
    "# x_col = ['speed', 'rush', 'grid_distance', 'delivery_MHD', 'target_MHD']\n",
    "\n",
    "\n",
    "t0 = time.time()\n",
    "model = lgb.LGBMRegressor(\n",
    "    metric = 'mae',\n",
    "    num_leaves = 64,\n",
    "    max_depth = 7,\n",
    "    n_estimators = 300,\n",
    "    learning_rate = 0.1,\n",
    "    bagging_fraction = 1,\n",
    "    feature_fraction = 0.8,\n",
    "    reg_alpha = 0,\n",
    "    reg_lambda = 0\n",
    ")\n",
    "\n",
    "valueK = 10\n",
    "oof = []\n",
    "df_importance_list = []\n",
    "\n",
    "kfold = KFold(n_splits=valueK, shuffle=True, random_state=2020)\n",
    "for fold_id, (trn_idx, val_idx) in enumerate(kfold.split(feature_train[x_col], feature_train[y_col])):\n",
    "    X_train = feature_train.iloc[trn_idx][x_col]\n",
    "    Y_train = feature_train.iloc[trn_idx][y_col]\n",
    "\n",
    "    X_val = feature_train.iloc[val_idx][x_col]\n",
    "    Y_val = feature_train.iloc[val_idx][y_col]\n",
    "\n",
    "    print('\\nFold{} Training ======================================\\n'.format(fold_id+1))\n",
    "\n",
    "    lgb_model = model.fit(\n",
    "        X_train,\n",
    "        Y_train,\n",
    "        eval_names=['train', 'valid'],\n",
    "        eval_set=[(X_train, Y_train), (X_val, Y_val)],\n",
    "        verbose=500,\n",
    "        eval_metric='mae',\n",
    "        early_stopping_rounds=100\n",
    "    )\n",
    "\n",
    "    pred_val = lgb_model.predict(X_val, num_iteration=lgb_model.best_iteration_)\n",
    "    df_oof = feature_train.iloc[val_idx][['id', y_col]].copy()\n",
    "    df_oof['pred'] = pred_val\n",
    "    oof.append(df_oof)\n",
    "\n",
    "    pred_test = lgb_model.predict(feature_test[x_col], num_iteration=lgb_model.best_iteration_)\n",
    "    reg_prediction['will_residue_time'] += (pred_test/valueK)\n",
    "\n",
    "    df_importance = pd.DataFrame({\n",
    "        'column': x_col,\n",
    "        'importance': lgb_model.feature_importances_\n",
    "    })\n",
    "    df_importance_list.append(df_importance)\n",
    "    # break\n",
    "\n",
    "    # del lgb_model, pred_val, pred_test, X_train, Y_train, X_val, Y_val\n",
    "    # gc.collect()\n",
    "t1 = time.time()\n",
    "print('end train, use{} second'.format(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oof = pd.concat(oof)\n",
    "mae = metrics.mean_absolute_error(df_oof[y_col], df_oof['pred'])\n",
    "print('mae:', mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 还原expect_time\n",
    "# reg_prediction['expect_time'] = ((reg_prediction['create_time']+reg_prediction['expect_used_time']) + (reg_prediction['promise_deliver_time']-reg_prediction['will_residue_time'])) / 2\n",
    "reg_prediction['expect_time'] = reg_prediction['promise_deliver_time'] - reg_prediction['will_residue_time']# 只使用后一个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train.to_pickle('./temp/regfuture_train_regend.pkl')\n",
    "reg_prediction.to_pickle('./temp/regfuture_test_regend.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分类特征"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time \n",
    "import datetime\n",
    "from tqdm import tqdm \n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import scipy\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing, metrics \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import shuffle# 随机排列\n",
    "import gc\n",
    "import lightgbm as lgb \n",
    "import xgboost as xgb \n",
    "import joblib\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train = pd.read_pickle('./temp/regfuture_train_regend.pkl')\n",
    "feature_test = pd.read_pickle('./temp/regfuture_test_regend.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征 deadline\n",
    "def deadLine(df):\n",
    "    df['deadline'] = df['promise_deliver_time'] - df['expect_time']\n",
    "    df['need_speed'] = df['grid_distance'] / df['deadline']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train = deadLine(feature_train)\n",
    "feature_test = deadLine(feature_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新包装了一个异常值处理函数，把异常值清洗设为50%中位数\n",
    "def deal_outliers(df, col):\n",
    "    # df = feature_train\n",
    "    # col = 'expect_used_time'\n",
    "    def Box_outliers(data_ser):\n",
    "        iqr = 3 * (data_ser.quantile(0.75) - data_ser.quantile(0.25))\n",
    "        val_low = data_ser.quantile(0.25) - iqr\n",
    "        val_up = data_ser.quantile(0.75) + iqr\n",
    "        return val_low, val_up\n",
    "\n",
    "    data_ser = df[col]\n",
    "    val_low, val_up = Box_outliers(df[col])\n",
    "    std_data = df[col].quantile(0.5)\n",
    "    df[col] = df[col].apply(lambda x: std_data if x < val_low else std_data if x > val_up else x)\n",
    "\n",
    "    sns.boxplot(y=df[col], data=df, palette=\"Set1\")\n",
    "    return df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train['need_speed'] = deal_outliers(feature_train, 'need_speed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train['deadline'] = deal_outliers(feature_train, 'deadline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train['deadline'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_Picked(df):\n",
    "    df['is_picked'] = df['last_time'] - df['estimate_pick_time']\n",
    "    df['is_picked'] = df['is_picked'].apply(lambda x: 0 if x<0 else 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_test = is_Picked(feature_test)\n",
    "feature_train = is_Picked(feature_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train.to_pickle('./temp/future_train_clf.pkl')\n",
    "feature_test.to_pickle('./temp/future_test_clf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分类模型"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目标：提高回归任务的mae，以提高expect_time预测精准度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time \n",
    "import datetime\n",
    "from tqdm import tqdm \n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import scipy\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing, metrics \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import shuffle# 随机排列\n",
    "import gc\n",
    "import lightgbm as lgb \n",
    "import xgboost as xgb \n",
    "import joblib\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train = pd.read_pickle('./temp/future_train_clf.pkl')\n",
    "feature_test = pd.read_pickle('./temp/future_test_clf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 临时特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_test['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train['train'] = 1\n",
    "feature_test['train'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = feature_train.append(feature_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in ['aoi_id', 'shop_id']:\n",
    "    print(f)\n",
    "    lbl = LabelEncoder()\n",
    "    feature[f] = lbl.fit_transform(feature[f].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train = feature[feature['train'] == 1].copy()\n",
    "feature_test = feature[feature['train'] == 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_Residue_Time(df):\n",
    "    df['total_residue_time'] = df['promise_deliver_time'] - df['estimate_pick_time']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train = total_Residue_Time(feature_train)\n",
    "feature_test = total_Residue_Time(feature_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train['label'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回到模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = feature_test[['courier_id', 'wave_index', 'tracking_id', 'courier_wave_start_lng', 'courier_wave_start_lat', 'action_type', 'expect_time', 'date', 'id', 'majorid', 'label']]\n",
    "prediction['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nFold_1 Training ================================\n\n"
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "DataFrame.dtypes for data must be int, float or bool.\nDid not expect the data types in the following fields: aoi_id, shop_id",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-162dbc4458a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0meval_metric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auc'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     )\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    831\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m                                         callbacks=callbacks, init_model=init_model)\n\u001b[0m\u001b[0;32m    834\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    616\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 618\u001b[1;33m                               callbacks=callbacks, init_model=init_model)\n\u001b[0m\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    226\u001b[0m     \u001b[1;31m# construct booster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         \u001b[0mbooster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBooster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_train_data_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, train_set, model_file, model_str, silent)\u001b[0m\n\u001b[0;32m   1780\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1781\u001b[0m             \u001b[1;31m# construct booster object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1782\u001b[1;33m             \u001b[0mtrain_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1783\u001b[0m             \u001b[1;31m# copy the parameters from train_set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1784\u001b[0m             \u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mconstruct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1154\u001b[0m                                 \u001b[0minit_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predictor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1155\u001b[0m                                 \u001b[0msilent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msilent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1156\u001b[1;33m                                 categorical_feature=self.categorical_feature, params=self.params)\n\u001b[0m\u001b[0;32m   1157\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfree_raw_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1158\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[0;32m    894\u001b[0m                                                                                              \u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    895\u001b[0m                                                                                              \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 896\u001b[1;33m                                                                                              self.pandas_categorical)\n\u001b[0m\u001b[0;32m    897\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_label_from_pandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_data_from_pandas\u001b[1;34m(data, feature_name, categorical_feature, pandas_categorical)\u001b[0m\n\u001b[0;32m    361\u001b[0m             raise ValueError(\"DataFrame.dtypes for data must be int, float or bool.\\n\"\n\u001b[0;32m    362\u001b[0m                              \u001b[1;34m\"Did not expect the data types in the following fields: \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m                              + ', '.join(data.columns[bad_indices]))\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: DataFrame.dtypes for data must be int, float or bool.\nDid not expect the data types in the following fields: aoi_id, shop_id"
     ]
    }
   ],
   "source": [
    "y_col = 'label'\n",
    "# x_col = ['grid_distance', 'target_tan', 'target_MHD', 'weather_grade', 'aoi_id', 'shop_id', 'delivery_tan', 'delivery_MHD', 'level', 'speed', 'max_load', 'rush', 'road', 'expect_used_time', 'will_residue_time', 'deadline', 'need_speed', 'is_picked']\n",
    "# x_col = ['grid_distance', 'target_tan', 'target_MHD', 'weather_grade', 'delivery_tan', 'delivery_MHD', 'level', 'speed', 'max_load', 'rush', 'road', 'expect_used_time', 'will_residue_time', 'deadline', 'need_speed', 'is_picked']# 939-911\n",
    "x_col = ['grid_distance', 'target_MHD', 'weather_grade', 'delivery_MHD', 'level', 'speed', 'max_load', 'rush', 'road', 'deadline', 'need_speed', 'is_picked']# \n",
    "# x_col = ['rush', 'target_MHD', 'delivery_MHD', 'grid_distance', 'max_load', 'is_picked']# 783-772-718\n",
    "# x_col = ['rush', 'target_MHD', 'delivery_MHD', 'grid_distance', 'max_load', 'is_picked', 'aoi_id', 'shop_id', 'courier_id']# 797-767-716\n",
    "\n",
    "\n",
    "model = lgb.LGBMClassifier(num_leaves=50,\n",
    "                           max_depth=6,\n",
    "                           learning_rate=0.1,\n",
    "                           n_estimators=1000,\n",
    "                           subsample=0.8,\n",
    "                           feature_fraction=0.8,\n",
    "                           reg_alpha=0,\n",
    "                           reg_lambda=1,\n",
    "                           )\n",
    "\n",
    "\n",
    "oof = []\n",
    "df_importance_list = []\n",
    "\n",
    "kfold = GroupKFold(n_splits=10)\n",
    "for fold_id, (trn_idx, val_idx) in enumerate(kfold.split(feature_train[x_col], feature_train[y_col], feature_train['majorid'])):\n",
    "    X_train = feature_train.iloc[trn_idx][x_col]\n",
    "    Y_train = feature_train.iloc[trn_idx][y_col]\n",
    "\n",
    "    X_val = feature_train.iloc[val_idx][x_col]\n",
    "    Y_val = feature_train.iloc[val_idx][y_col]\n",
    "\n",
    "    print('\\nFold_{} Training ================================\\n'.format(fold_id+1))\n",
    "\n",
    "    lgb_model = model.fit(\n",
    "        X_train,\n",
    "        Y_train,\n",
    "        eval_names=['train', 'valid'],\n",
    "        eval_set=[(X_train, Y_train), (X_val, Y_val)],\n",
    "        verbose=200,\n",
    "        eval_metric='auc',\n",
    "        early_stopping_rounds=100\n",
    "    )\n",
    "\n",
    "    pred_val = lgb_model.predict_proba(X_val, num_iteration=lgb_model.best_iteration_)[:, 1]\n",
    "    df_oof = feature_train.iloc[val_idx][['id', 'majorid', y_col]].copy()\n",
    "    df_oof['pred'] = pred_val\n",
    "    oof.append(df_oof)\n",
    "\n",
    "    pred_test = lgb_model.predict_proba(feature_test[x_col], num_iteration=lgb_model.best_iteration_)[:, 1]\n",
    "    prediction['label'] += pred_test / 10\n",
    "\n",
    "    df_importance = pd.DataFrame({\n",
    "        'column': x_col,\n",
    "        'importance': lgb_model.feature_importances_,\n",
    "    })\n",
    "    df_importance_list.append(df_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance = pd.concat(df_importance_list)\n",
    "df_importance = df_importance.groupby(['column'])['importance'].agg('mean').sort_values(ascending=False).reset_index()\n",
    "df_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证recall，对feature_train 和 oof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_func(majorid):\n",
    "    majorid = majorid.values.tolist()\n",
    "    max_index = majorid.index(max(majorid))\n",
    "    result = np.zeros(len(majorid))\n",
    "    result[max_index] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Acc(df):\n",
    "    TP = df[(df['label']==1) & (df['result']==1)].shape[0] # 预测正确，预测的值为真\n",
    "    TN = df[(df['label']==0) & (df['result']==0)].shape[0] # 预测正确，预测的值为假\n",
    "    FP = df[(df['label']==0) & (df['result']==1)].shape[0] # 预测错误，预测的值为真\n",
    "    FN = df[(df['label']==1) & (df['result']==0)].shape[0] # 预测错误，预测的值为假\n",
    "    TP_FN = df[df['label']==1].shape[0]\n",
    "    TP_FP = df[df['result']==1].shape[0]\n",
    "    print('recall:{}'.format(TP/TP_FN))\n",
    "    print('acc:{}'.format(TP/TP_FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train['pred_val'] = lgb_model.predict_proba(feature_train[x_col], num_iteration=lgb_model.best_iteration_)[:, 1]\n",
    "feature_train['result'] = feature_train.groupby(['majorid'])['pred_val'].transform(result_func)\n",
    "Acc(feature_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oof = pd.concat(oof)\n",
    "df_oof['result'] = df_oof.groupby(['majorid'])['pred'].transform(result_func)\n",
    "Acc(df_oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_func(majorid):\n",
    "    majorid = majorid.values.tolist()\n",
    "    max_index = majorid.index(max(majorid))\n",
    "    result = np.zeros(len(majorid))\n",
    "    result[max_index] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction['rusult'] = prediction.groupby(['majorid'])['label'].transform(result_func)\n",
    "subfile = prediction[prediction['rusult'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = subfile[['courier_id', 'wave_index', 'tracking_id', 'courier_wave_start_lng', 'courier_wave_start_lat', 'action_type', 'expect_time', 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "os.makedirs('./sub/{}'.format('result'), exist_ok=True)\n",
    "f = zipfile.ZipFile('./sub/{}.zip'.format('result'), 'w', zipfile.ZIP_DEFLATED)\n",
    "for date in result['date'].unique():\n",
    "    df_temp = result[result['date'] == date]\n",
    "    del df_temp['date']\n",
    "    df_temp.to_csv('./sub/{}/action_{}.txt'.format('result', date), index=False)\n",
    "    f.write('./sub/{}/action_{}.txt'.format('result', date), 'action_{}.txt'.format(date))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}